{
    "dataset_reader": {
        "type": "nabert++",
        "tokenizer": {
            "type": "bert-drop",
            "pretrained_model": "./chinese_roberta_wwm_ext_pytorch/vocab.txt"
        },
        "token_indexers": {
          "tokens": {
            "type": "bert-drop",
            "pretrained_model": "./chinese_roberta_wwm_ext_pytorch/vocab.txt"
          }
        },
        "bio_types": ["multiple_span", "single_span", "number", "date"],
        "extra_numbers": [100, 1],
        "is_training": true,
        "wordpiece_numbers": false,
        "flexibility_threshold": 1000
    },
    "validation_dataset_reader": {
        "type": "nabert++",
        "tokenizer": {
            "type": "bert-drop",
            "pretrained_model": "./chinese_roberta_wwm_ext_pytorch/vocab.txt"
        },
        "token_indexers": {
          "tokens": {
            "type": "bert-drop",
            "pretrained_model": "./chinese_roberta_wwm_ext_pytorch/vocab.txt"
          }
        },
        "bio_types": ["multiple_span", "single_span", "number", "date"],
        "extra_numbers": [100, 1],
        "is_training": false,
        "wordpiece_numbers": false,
        "flexibility_threshold": 1000
    },
    "iterator": {
        "type": "basic",
        "batch_size": 4
    },
    "model": {
        "type": "nabert++",
        "bert_pretrained_model": "./chinese_roberta_wwm_ext_pytorch/",
        "dropout_prob": 0.1,
        "special_numbers": [ 100, 1],
        "answering_abilities": ["passage_span_extraction", "arithmetic", "counting", "multiple_spans", "yesno"]
    },
    "train_data_path": "data/zh/123_A.json",
    "validation_data_path": "data/zh/123_B.json",
    "trainer": {
        "cuda_device": 0,
        "keep_serialized_model_every_num_seconds": 3600,
        "num_epochs": 20,
        "optimizer": {
            "type": "bert_adam",
            "lr": 1e-05
        },
        "patience": 10,
        "summary_interval": 100,
        "validation_metric": "+f1"
    }
}